{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78df25e9-a97a-418c-a573-9c3d4d07fa0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "from itertools import compress\n",
    "import os.path as op\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from mne.io import read_raw_nirx as nirx\n",
    "from copy import deepcopy\n",
    "import os\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90badd82-dba6-48d5-81bb-51f35086482e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GLOBAL VARIABLE: LIST OF SHORT CHANNELS\n",
    "\n",
    "# These are broadly uninteresting to hyperscanning analysis.\n",
    "ALL_SHORT_CHANNELS = [\n",
    " 'S3_D23 hbo',\n",
    " 'S3_D23 hbr',\n",
    " 'S4_D24 hbo',\n",
    " 'S4_D24 hbr',\n",
    " 'S5_D31 hbo',\n",
    " 'S5_D31 hbr',\n",
    " 'S8_D32 hbo',\n",
    " 'S8_D32 hbr',\n",
    " 'S10_D25 hbo',\n",
    " 'S10_D25 hbr',\n",
    " 'S11_D26 hbo',\n",
    " 'S11_D26 hbr',\n",
    " 'S12_D27 hbo',\n",
    " 'S12_D27 hbr',\n",
    " 'S13_D28 hbo',\n",
    " 'S13_D28 hbr',\n",
    " 'S15_D29 hbo',\n",
    " 'S15_D29 hbr',\n",
    " 'S16_D30 hbo',\n",
    " 'S16_D30 hbr',\n",
    " 'S18_D33 hbo',\n",
    " 'S18_D33 hbr',\n",
    " 'S19_D34 hbo',\n",
    " 'S19_D34 hbr',\n",
    " 'S20_D35 hbo',\n",
    " 'S20_D35 hbr',\n",
    " 'S21_D36 hbo',\n",
    " 'S21_D36 hbr',\n",
    " 'S23_D37 hbo',\n",
    " 'S23_D37 hbr',\n",
    " 'S24_D38 hbo',\n",
    " 'S24_D38 hbr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21c24498-aeca-41d7-9e3d-8d721ec98791",
   "metadata": {},
   "outputs": [],
   "source": [
    "####### FUNCTIONS #########\n",
    "\n",
    "\n",
    "# Given list of channels (Such as irrelevant short channels), drop them.\n",
    "def filter_relevant_channels(fnirs_data):\n",
    "    fnirs_data.drop_channels(ALL_SHORT_CHANNELS)\n",
    "\n",
    "# Changes to THIS function will allow us to subdivide conversations if we wish.\n",
    "def fix_convo_annotations(raw_intensity):\n",
    "    raw_intensity.annotations.rename({'9.0': 'rest',\n",
    "                                  '11.0' : 'Convo2',\n",
    "                                  '10.0': 'Convo1'})\n",
    "    for i in range(30): # Remove all unchosen triggers.\n",
    "        raw_intensity.annotations.delete(raw_intensity.annotations.description == f'{i}.0')\n",
    "    raw_intensity.annotations.duration = np.array([120, 300, 60, 300, 120])\n",
    "    return raw_intensity\n",
    "\n",
    "\n",
    "# Quite simply, load the NIRSport2 data given a path, and apply fix_convo_annotations.    \n",
    "def load_nirx_from_input_path(input_path):\n",
    "    data = nirx(input_path, verbose = 'CRITICAL')\n",
    "    raw_intensity = data.load_data()\n",
    "    loaded_data = fix_convo_annotations(raw_intensity)\n",
    "    return loaded_data\n",
    "\n",
    "# Converts first to optical density, computes the SCI, and interpolates if necessary.\n",
    "def convert_to_haemoglobin_and_interpolate(loaded_data, sci_threshold = 0.6):\n",
    "    raw_optical_density = mne.preprocessing.nirs.optical_density(loaded_data)\n",
    "    sci = mne.preprocessing.nirs.scalp_coupling_index(raw_optical_density)\n",
    "    raw_optical_density.info['bads'] = list(compress(raw_optical_density.ch_names, sci < sci_threshold))\n",
    "    raw_optical_density.interpolate_bads(reset_bads = True, method = dict(fnirs = 'nearest'))\n",
    "    raw_haemoglobin = mne.preprocessing.nirs.beer_lambert_law(raw_optical_density, ppf=6)\n",
    "    return raw_haemoglobin\n",
    "\n",
    "# Helper function, because MNE's built in pick_channels() function edits the original object in place!\n",
    "def pick_channels_deepcopy(raw_haemoglobin, channel_name):\n",
    "    haemoglobin_copy = deepcopy(raw_haemoglobin)\n",
    "    single_channel_data = haemoglobin_copy.pick_channels([channel_name], verbose = False)\n",
    "    return single_channel_data\n",
    "\n",
    "# Because we didn't use the hyperscan application, we need to use triggers to make time series \"start\" at the same time.\n",
    "def truncate_time_series(single_channel_data, rounding, verbose = False):\n",
    "\n",
    "    # Useful for debugging\n",
    "    if verbose:\n",
    "        print(single_channel_data.times.shape)\n",
    "        print(single_channel_data._data[0].shape)\n",
    "\n",
    "    # Get triggers.\n",
    "    triggers = single_channel_data.annotations.onset\n",
    "    # Find earliest onset\n",
    "    earliest_onset = np.round(triggers[0], rounding)\n",
    "\n",
    "    # Sanity check: is this onset actually in the times vector?\n",
    "    assert earliest_onset in np.round(single_channel_data.times, rounding)\n",
    "\n",
    "    # Figure out at what exact measurement the experiment starts. This should be greater than 0.\n",
    "    experiment_start_idx = np.where(np.round(single_channel_data.times, rounding) == earliest_onset)[0][0]\n",
    "    assert experiment_start_idx >= 0\n",
    "\n",
    "    # Figure out when last trigger (should be a rest trigger) occured in the recording.\n",
    "    last_onset = np.round(triggers[-1], rounding)\n",
    "    assert last_onset in np.round(single_channel_data.times, rounding)\n",
    "    \n",
    "    # Experiment ends APPROXIMATELY at following index:\n",
    "    experiment_end_idx = np.where(np.round(single_channel_data.times, rounding) == last_onset)[0][0] + 611\n",
    "\n",
    "    #Truncate measurement times to match experiment start and end.\n",
    "    output_timeseries = single_channel_data.times[experiment_start_idx:experiment_end_idx]\n",
    "    # Truncate actual measurements to match experiment start and end.\n",
    "    output_data = single_channel_data._data[0][experiment_start_idx:experiment_end_idx]\n",
    "\n",
    "    # Re-configure times vector to new zero\n",
    "    output_timeseries =  output_timeseries - output_timeseries[0]\n",
    "\n",
    "    # Re-configure trigger onset times to new zero\n",
    "    triggers = triggers - triggers[0]\n",
    "\n",
    "    # A bunch more sanity checks\n",
    "    assert len({i >= 0 for i in output_timeseries}) == 1\n",
    "    assert output_timeseries[1] > 0\n",
    "    assert output_timeseries[0] == 0\n",
    "    assert triggers[1] > 0\n",
    "    assert triggers[0] == 0\n",
    "    if verbose:\n",
    "        print(output_timeseries.shape)\n",
    "        print(output_data.shape)\n",
    "    return output_timeseries, output_data, triggers\n",
    "\n",
    "# Saves a time series (That is, a vector of measurements and measurement times) plus associated triggers.\n",
    "def save_time_series(output_timeseries, output_data, triggers, output_path, filename):\n",
    "    data_dict = { \"c1\"  : output_timeseries,\"c2\" : list(output_data)}\n",
    "    triggers_dict = {\"triggers\" : triggers}\n",
    "    # Turn times and measurements into dataframe\n",
    "    pd.DataFrame(data = data_dict).to_csv(output_path +\"/\"+ filename  +\".tsv\", sep=\"\\t\", index=None, header = False)\n",
    "    # Save triggers (identical across all channels, need only be saved once)\n",
    "    if not os.path.exists(output_path +\"/\" + \"triggers.tsv\"):\n",
    "        pd.DataFrame(data = triggers_dict).to_csv(output_path +\"/\" + \"_triggers.tsv\", sep=\"\\t\", index=None, header = False)\n",
    "\n",
    "# Function that bundles the whole thing together.\n",
    "def nirx_to_timeseries(input_path, output_path, participant_nr, verbose = False):\n",
    "    try:\n",
    "        loaded_data = load_nirx_from_input_path(input_path)\n",
    "    except FileNotFoundError:\n",
    "        print(\"WARNING: \" + input_path + \" not found.\")\n",
    "        return\n",
    "    raw_haemoglobin = convert_to_haemoglobin_and_interpolate(loaded_data)\n",
    "    filter_relevant_channels(raw_haemoglobin)\n",
    "    channel_list = raw_haemoglobin.ch_names\n",
    "    for channel_name in channel_list:\n",
    "        filename =  f\"{participant_nr}\" + \"_\" + channel_name\n",
    "        single_channel_data = pick_channels_deepcopy(raw_haemoglobin, channel_name)\n",
    "        output_timeseries, output_data, triggers = truncate_time_series(single_channel_data, rounding = 10, verbose = verbose)\n",
    "        save_time_series(output_timeseries, output_data, triggers, output_path, filename)\n",
    "\n",
    "# Helper function which creates new directories.\n",
    "def make_folder(newpath):\n",
    "    if not os.path.exists(newpath):\n",
    "        os.makedirs(newpath)\n",
    "\n",
    "\n",
    "# FUNCTION FOR GETTING AN OVERVIEW OF BAD CHANNELS\n",
    "\n",
    "#https://mne.discourse.group/t/interpolation-of-bad-channels-in-fnirs-data/4100/5\n",
    "\n",
    "#https://mne.tools/stable/auto_tutorials/preprocessing/15_handling_BadChannels.html\n",
    "\n",
    "def check_bads(input_path, plot_name):\n",
    "    make_folder(\"Plots/BadChannelsPlots\")\n",
    "    try:\n",
    "        loaded_data = load_nirx_from_input_path(input_path)\n",
    "    except FileNotFoundError:\n",
    "        print(\"WARNING: \" + input_path + \" not found.\")\n",
    "        return\n",
    "    raw_optical_density = mne.preprocessing.nirs.optical_density(loaded_data)\n",
    "    sci = mne.preprocessing.nirs.scalp_coupling_index(raw_optical_density)\n",
    "    # Specify SCI cutoff here\n",
    "    indeces = [i for i, v in enumerate(sci) if v < 0.4]\n",
    "    raw_haemoglobin = mne.preprocessing.nirs.beer_lambert_law(raw_optical_density, ppf=6)\n",
    "    \n",
    "    BadChannels = [raw_haemoglobin.ch_names[i] for i in indeces]\n",
    "    BadChannels = [i for i in BadChannels if i not in ALL_SHORT_CHANNELS]\n",
    "    fig, ax = plt.subplots(layout=\"constrained\")\n",
    "    ax.hist(sci)\n",
    "    ax.set(xlabel=\"Scalp Coupling Index\", ylabel=\"Count\", xlim=[0, 1])\n",
    "    \n",
    "    plt.savefig(\"Plots/BadChannelsPlots\" + f\"/{plot_name}_bad_channel_plot.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    return BadChannels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366ff454-8037-4e79-949d-15f4602d2155",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Check bad channels\n",
    "\n",
    "pair_input_paths = [\"../Data/AllParticipants\" + \"/\" + folder for folder in os.listdir(\"../Data/AllParticipants\")]\n",
    "for pair_number, pair_path in enumerate(pair_input_paths):\n",
    "    \n",
    "    print(pair_number)\n",
    "    if pair_number == 15: # Pair 15 isn't complete.\n",
    "        continue\n",
    "\n",
    "    data1_visit1 = check_bads( pair_path + \"/Visit1/Participant1/convo\", plot_name = f\"{pair_number}_p1_v1\")\n",
    "    data2_visit1 = check_bads( pair_path + \"/Visit1/Participant2/convo\", plot_name = f\"{pair_number}_p2_v1\")\n",
    "    ## Visit 4\n",
    "    data1_visit4 =  check_bads( pair_path + \"/Visit4/Participant1/convo\",plot_name = f\"{pair_number}_p1_v4\")\n",
    "    data2_visit4 = check_bads( pair_path + \"/Visit4/Participant2/convo\", plot_name = f\"{pair_number}_p1_v4\")\n",
    "    pd.DataFrame(data = data1_visit1).to_csv(\"../Data/AllParticipants/BadChannelsLists/\" f\"{pair_number}\" + \"_BadChannels_v1_p1.tsv\", sep=\"\\t\", index=None, header = False)\n",
    "    pd.DataFrame(data = data2_visit1).to_csv(\"../Data/AllParticipants/BadChannelsLists/\" f\"{pair_number}\" + \"_BadChannels_v1_p2.tsv\", sep=\"\\t\", index=None, header = False)\n",
    "    pd.DataFrame(data = data1_visit4).to_csv(\"../Data/AllParticipants/BadChannelsLists/\" f\"{pair_number}\" + \"_BadChannels_v4_p1.tsv\", sep=\"\\t\", index=None, header = False)\n",
    "    pd.DataFrame(data = data2_visit4).to_csv(\"../Data/AllParticipants/BadChannelsLists/\" f\"{pair_number}\" + \"_BadChannels_v4_p2.tsv\", sep=\"\\t\", index=None, header = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5356016-e6a9-4393-aaba-c21937542403",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_folder(\"HaemoglobinTimeSeries\")\n",
    "\n",
    "# Now that bad channels have been checked and plotted, load all the NIRSport2 data and sae as csv-based timeline files.\n",
    "\n",
    "pair_input_paths = [\"../Data/AllParticipants\" + \"/\" + folder for folder in os.listdir(\"..\\Data\\AllParticipants\")]\n",
    "for pair_number, pair_path in enumerate(pair_input_paths[1:23]):\n",
    "    print(pair_number + 1)\n",
    "    if pair_number +1 == 15: # Pair 15 isn't complete.\n",
    "        continue\n",
    "        \n",
    "    # Create output directories\n",
    "    output_path_visit1 = \"HaemoglobinTimeSeries\" + f\"/{pair_number + 1}\" + \"/Visit1\"\n",
    "    make_folder(output_path_visit1)\n",
    "    output_path_visit4 = \"HaemoglobinTimeSeries\" + f\"/{pair_number + 1}\" + \"/Visit4\"\n",
    "    make_folder(output_path_visit4)\n",
    "    \n",
    "    # Load data and save to appropriate output directory\n",
    "\n",
    "    ## Visit 1\n",
    "    nirx_to_timeseries(input_path = pair_path + \"/Visit1/Participant1/convo\", output_path = output_path_visit1, participant_nr = 1)\n",
    "    nirx_to_timeseries(input_path = pair_path + \"/Visit1/Participant2/convo\", output_path = output_path_visit1, participant_nr = 2)\n",
    "    ## Visit 4\n",
    "    nirx_to_timeseries(input_path = pair_path + \"/Visit4/Participant1/convo\", output_path = output_path_visit4, participant_nr = 1)\n",
    "    nirx_to_timeseries(input_path = pair_path + \"/Visit4/Participant2/convo\", output_path = output_path_visit4, participant_nr = 2)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
